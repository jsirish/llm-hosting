#!/bin/bash
# LiteLLM Proxy Setup for RunPod
# Creates a unified proxy to normalize vLLM responses for Copilot compatibility

set -e

echo "ðŸ”§ Installing LiteLLM..."
pip install 'litellm[proxy]'

echo "â„¹ï¸  Running without database (simpler setup, no Prisma needed)"

echo "ðŸ“ Creating LiteLLM config..."
cat > /workspace/litellm-config.yaml << 'EOF'
model_list:
  # Gemma 3 27B FP8 (Primary - 128K context)
  - model_name: gemma-3-27b
    litellm_params:
      model: openai/leon-se/gemma-3-27b-it-FP8-Dynamic
      api_base: http://localhost:8000/v1
      api_key: sk-vllm-4621df5fb0a1f78adbe91ff2fbca9cd9
    model_info:
      mode: chat
      supports_function_calling: true
      supports_vision: true
      max_tokens: 131072

  # Qwen 30B FP8 (Fallback - 128K context)
  - model_name: qwen-30b
    litellm_params:
      model: openai/Qwen/Qwen3-Coder-30B-A3B-Instruct-FP8
      api_base: http://localhost:8000/v1
      api_key: sk-vllm-4621df5fb0a1f78adbe91ff2fbca9cd9
    model_info:
      mode: chat
      supports_function_calling: true
      supports_vision: false
      max_tokens: 131072

  # GPT-OSS 20B (Alternative - 128K context)
  - model_name: gpt-oss-20b
    litellm_params:
      model: openai/openai/gpt-oss-20b
      api_base: http://localhost:8000/v1
      api_key: sk-vllm-4621df5fb0a1f78adbe91ff2fbca9cd9
    model_info:
      mode: chat
      supports_function_calling: true
      supports_vision: false
      max_tokens: 131072

litellm_settings:
  drop_params: true  # Strip non-standard parameters
  json_logs: true
  num_retries: 2
  request_timeout: 600
  modify_params: true
  allowed_fails: 1
  guardrails: false  # Disable strict validation
  success_callback: []

general_settings:
  master_key: sk-vllm-4621df5fb0a1f78adbe91ff2fbca9cd9
EOF

echo "ðŸ“„ Creating LiteLLM start script..."
cat > /workspace/start-litellm.sh << 'EOF'
#!/bin/bash
# Start LiteLLM proxy on port 4000
# Normalizes vLLM responses for Copilot compatibility

echo "ðŸš€ Starting LiteLLM proxy..."
echo "Port: 4000"
echo ""

# IMPORTANT: Remove old config that might have database_url
echo "ðŸ“ Regenerating config to remove any database settings..."
cat > /workspace/litellm-config.yaml << 'INNER_EOF'
model_list:
  - model_name: gemma-3-27b
    litellm_params:
      model: openai/leon-se/gemma-3-27b-it-FP8-Dynamic
      api_base: http://localhost:8000/v1
      api_key: sk-vllm-4621df5fb0a1f78adbe91ff2fbca9cd9
    model_info:
      mode: chat
      supports_function_calling: true
      supports_vision: true
      max_tokens: 131072

  - model_name: qwen-30b
    litellm_params:
      model: openai/Qwen/Qwen3-Coder-30B-A3B-Instruct-FP8
      api_base: http://localhost:8000/v1
      api_key: sk-vllm-4621df5fb0a1f78adbe91ff2fbca9cd9
    model_info:
      mode: chat
      supports_function_calling: true
      supports_vision: false
      max_tokens: 131072

  - model_name: gpt-oss-20b
    litellm_params:
      model: openai/openai/gpt-oss-20b
      api_base: http://localhost:8000/v1
      api_key: sk-vllm-4621df5fb0a1f78adbe91ff2fbca9cd9
    model_info:
      mode: chat
      supports_function_calling: true
      supports_vision: false
      max_tokens: 131072

litellm_settings:
  drop_params: true
  json_logs: true
  num_retries: 2
  request_timeout: 600
  modify_params: true
  allowed_fails: 1
  guardrails: false  # Disable strict validation
  success_callback: []

general_settings:
  master_key: sk-vllm-4621df5fb0a1f78adbe91ff2fbca9cd9
INNER_EOF

echo "âœ… Config regenerated without database"
echo ""

# Start without database (simpler, no Prisma dependency)
litellm --config /workspace/litellm-config.yaml \
  --port 4000 \
  --host 0.0.0.0 \
  --detailed_debug
EOF

chmod +x /workspace/start-litellm.sh

echo ""
echo "âœ… LiteLLM setup complete!"
echo ""
echo "ðŸ“‹ Usage:"
echo "  1. Start vLLM model: ./gemma3.sh (or ./qwen.sh)"
echo "  2. In new terminal, start LiteLLM: ./start-litellm.sh"
echo "  3. Update VS Code to use LiteLLM endpoint"
echo ""
echo "ðŸŒ Endpoints:"
echo "  Direct vLLM: https://v5brcrgoclcp1p-8000.proxy.runpod.net/v1"
echo "  LiteLLM Proxy: https://v5brcrgoclcp1p-4000.proxy.runpod.net/v1"
echo ""
echo "ðŸ”‘ API Key (same for both):"
echo "  sk-vllm-4621df5fb0a1f78adbe91ff2fbca9cd9"
echo ""
echo "âœ¨ Features:"
echo "  - Strips reasoning/reasoning_content fields"
echo "  - Strips annotations, audio, refusal fields"
echo "  - Full OpenAI API compatibility for Copilot"
echo "  - Usage tracking in SQLite DB"
