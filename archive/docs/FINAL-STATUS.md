# üéâ Final Deployment Status - Qwen3-Coder-30B on RunPod

**Date**: December 11, 2025
**Status**: ‚úÖ **PRODUCTION READY**

---

## üöÄ Current Configuration

### Server Details
- **Model**: Qwen/Qwen3-Coder-30B-A3B-Instruct-FP8
- **Parameters**: 30 billion (pre-quantized FP8)
- **Context Window**: 131,072 tokens (128K - maximized!)
- **GPU**: NVIDIA RTX 6000 Ada (48GB VRAM)
- **Platform**: RunPod Cloud
- **Cost**: $0.78/hour
- **vLLM Version**: 0.12.0

### API Access
- **Endpoint**: `https://v5brcrgoclcp1p-8000.proxy.runpod.net/v1`
- **Current API Key**: `sk-vllm-c627cedbf339782f52774e32377d84b6`
- **Authentication**: Bearer token (header: `Authorization: Bearer <key>`)

### Tool Calling Configuration
```bash
--enable-auto-tool-choice
--tool-call-parser qwen3_coder
```
Uses Qwen's native function calling format (JSON)

### Server Management (New Simplified Architecture!)
```bash
# Start Qwen with 128K context
./qwen.sh

# Start GPT-OSS with 128K context (when available)
./gptoss.sh

# Stop server
./stop-server.sh
```
See `SERVER-MANAGEMENT.md` for full documentation

---

## ‚úÖ What Works

### Confirmed Working
- ‚úÖ Server deployment and startup
- ‚úÖ Model loading (15GB download, 30GB VRAM)
- ‚úÖ OpenAI-compatible API
- ‚úÖ Bearer token authentication
- ‚úÖ Code generation (high quality)
- ‚úÖ 32k context window
- ‚úÖ FP8 quantization
- ‚úÖ Tool calling enabled (native format)

### Tested Features
- Code completion (Fibonacci, string reversal)
- Multi-language support
- Comments and documentation
- Multiple solution approaches
- Complexity analysis

---

## üîß Configuration Journey

### What Didn't Work
1. ‚ùå **GPT-OSS-20B**: Unavailable PyTorch dependencies (torch==2.9.0.dev20250804+cu128)
2. ‚ùå **Initial cache location**: Filled up 30GB root overlay
3. ‚ùå **Hermes tool parser**: Generated XML format instead of JSON
4. ‚ùå **No tool parser**: Required by vLLM with --enable-auto-tool-choice

### Solutions Implemented
1. ‚úÖ **Switched to Qwen3-Coder-30B-FP8**: Latest model, pre-quantized, works with standard vLLM
2. ‚úÖ **Moved cache to /workspace/hf-cache**: Uses 304TB network storage instead of 30GB overlay
3. ‚úÖ **Used default parser**: Qwen's native JSON function calling format
4. ‚úÖ **Cleared 49GB cache**: Freed space by removing old downloads

---

## üìù Quick Reference

### Connect to Pod
```bash
./connect-runpod.sh
# Or:
ssh v5brcrgoclcp1p-64411a48@ssh.runpod.io -i ~/.ssh/runpod_ed25519
```

### Server Management (on pod)
```bash
./start-server.sh              # Start server
./stop-server.sh               # Stop server
./check-server.sh              # Check status
tail -f /workspace/logs/vllm-server.log  # Monitor logs
cat /workspace/logs/api-key.txt          # Get API key
```

### Test API (from Mac)
```bash
# Set environment
export VLLM_API_KEY="sk-vllm-c627cedbf339782f52774e32377d84b6"

# Health check
curl https://v5brcrgoclcp1p-8000.proxy.runpod.net/health \
  -H "Authorization: Bearer ${VLLM_API_KEY}"

# List models
curl https://v5brcrgoclcp1p-8000.proxy.runpod.net/v1/models \
  -H "Authorization: Bearer ${VLLM_API_KEY}"

# Chat completion
curl https://v5brcrgoclcp1p-8000.proxy.runpod.net/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer ${VLLM_API_KEY}" \
  -d '{
    "model": "Qwen/Qwen3-Coder-30B-A3B-Instruct-FP8",
    "messages": [{"role": "user", "content": "Write a Python function"}],
    "max_tokens": 500
  }'
```

---

## üîó GitHub Copilot Integration

### Current Status
‚ö†Ô∏è **Testing in progress** - Tool calling format may need adjustment

### VS Code Setup Options

#### Option 1: Continue.dev Extension (Recommended)
```json
{
  "models": [{
    "title": "Qwen3-Coder-30B",
    "provider": "openai",
    "model": "Qwen/Qwen3-Coder-30B-A3B-Instruct-FP8",
    "apiBase": "https://v5brcrgoclcp1p-8000.proxy.runpod.net/v1",
    "apiKey": "sk-vllm-c627cedbf339782f52774e32377d84b6",
    "contextLength": 131072
  }]
}
```

#### Option 2: Native GitHub Copilot
GitHub Copilot doesn't natively support custom models. Use Continue.dev or proxy setup.

### Known Issues
- Tool calling format compatibility with Copilot Chat being tested
- May require parser adjustments if tool calls don't work properly

---

## üìä Performance & Cost

### Resource Usage
- **Model Size**: 15GB download, ~30GB VRAM
- **GPU Memory**: ~42GB total (model + KV cache at 0.95 utilization)
- **Disk Space**: 15GB in /workspace/hf-cache
- **Network**: 304TB available

### Cost Analysis
- **Per Hour**: $0.78
- **Per Day (24/7)**: $18.72
- **Per Month (24/7)**: $561.60
- **Work Hours Only** (8hrs/day, 5 days/week): ~$125/month

### Optimization Strategies
1. Run only during active work hours
2. Use RunPod spot pricing (lower cost, can be interrupted)
3. Switch to smaller model if quality is acceptable
4. Auto-stop when idle (requires custom script)

---

## üõ†Ô∏è Troubleshooting

### Server Won't Start
```bash
# Check logs
tail -50 /workspace/logs/vllm-server.log

# Check disk space
df -h

# Clear cache if needed
rm -rf /workspace/.cache/huggingface

# Check process
ps aux | grep vllm
```

### Out of Disk Space
```bash
# Check usage
du -sh /workspace/*
du -sh /workspace/hf-cache/*

# Clear old cache
rm -rf /workspace/.cache/huggingface

# Clear temp files
rm -rf /tmp/torchinductor_root
rm -rf /tmp/tiktoken-rs-cache
```

### Tool Calling Issues
If GitHub Copilot shows tool call errors, try different parsers:
```bash
# Current: default (Qwen native JSON)
--tool-call-parser default

# Alternatives to try:
--tool-call-parser hermes     # Hermes format
--tool-call-parser granite    # IBM Granite format
--tool-call-parser mistral    # Mistral format
```

Edit in `start-server.sh`, line 65, restart server.

---

## üìö Documentation Files

- **DEPLOYMENT-SUCCESS.md**: Full deployment details and configuration
- **VSCODE-SETUP.md**: VS Code/Continue.dev setup instructions
- **QUICK-REFERENCE.md**: Quick commands and endpoints
- **start-server.sh**: Main startup script (on pod)
- **GPT-OSS-20B_DigitalOcean_0.76hr.md**: Initial research notes

---

## üéØ Next Steps

### Immediate
1. ‚úÖ Test tool calling with GitHub Copilot
2. ‚è≥ Verify tool call format compatibility
3. ‚è≥ Adjust parser if needed

### Short Term
1. Test 32k context window with large files
2. Benchmark inference speed
3. Compare quality vs GPT-4/Claude
4. Set up cost monitoring

### Long Term
1. Explore cost optimization (spot pricing, auto-stop)
2. Try alternative models for comparison
3. Set up monitoring/alerting
4. Document best practices

---

## üîë Key Learnings

1. **Pre-quantized models are better**: FP8 suffix models download faster and load reliably
2. **Watch disk space**: RunPod containers have limited overlay storage (30GB)
3. **Use workspace storage**: /workspace has 304TB network storage
4. **Clear old caches**: Old model downloads can accumulate quickly (49GB!)
5. **Native tool calling**: Use model's native format (default parser) for best compatibility
6. **API keys regenerate**: New key on every server restart

---

## üìû Support Resources

- **RunPod**: https://docs.runpod.io/
- **vLLM**: https://docs.vllm.ai/
- **Qwen**: https://huggingface.co/Qwen
- **Continue.dev**: https://continue.dev/docs

---

**Last Updated**: December 11, 2025
**Current API Key**: `sk-vllm-c627cedbf339782f52774e32377d84b6`
**Context Window**: 128K tokens (131,072)
**Status**: Production Ready ‚úÖ
