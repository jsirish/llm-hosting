#!/bin/bash
# Kimi K2 (DeepSeek-V3) Chat Model - vLLM Server Launcher
# Port: 8000 | Model: ~30B parameters | VRAM: ~45GB | Context: 128K tokens
# Use with: Continue.dev, Copilot, or any OpenAI-compatible client

set -e

echo "ðŸš€ Starting Kimi K2 Chat Model (DeepSeek-V3)..."
echo "==============================================="

# Configuration
# HuggingFace Token (for gated models)
# Set via: export HF_TOKEN="your_token_here" or run scripts/setup-hf-token.sh
export HF_TOKEN="${HF_TOKEN:-}"
export VLLM_MODEL="deepseek-ai/DeepSeek-V3"
export VLLM_SERVED_MODEL_NAME="kimi-k2"
export VLLM_PORT=8000
export VLLM_HOST="0.0.0.0"
export VLLM_MAX_MODEL_LEN=131072  # 128K tokens
export VLLM_GPU_MEMORY_UTIL=0.95  # Use 95% of VRAM (~45GB on RTX 6000 Ada)
export VLLM_TOOL_PARSER="deepseek"
# API Key will be generated by start-vllm-server.sh if not set
export VLLM_API_KEY="${VLLM_API_KEY:-}"

# Performance optimizations
export VLLM_TOKENIZER_MODE="auto"
export VLLM_ENABLE_PREFIX_CACHING="true"

# Cache configuration
export HF_HOME="/workspace/hf-cache"
export HF_HUB_CACHE="/workspace/hf-cache"

# Logging
LOG_DIR="/workspace/logs"
mkdir -p "$LOG_DIR"
LOG_FILE="$LOG_DIR/kimi-k2.log"

echo "ðŸ“‹ Configuration:"
echo "   Model: $VLLM_MODEL"
echo "   Served as: $VLLM_SERVED_MODEL_NAME"
echo "   Port: $VLLM_PORT"
echo "   Context Length: $VLLM_MAX_MODEL_LEN tokens"
echo "   Tool Parser: $VLLM_TOOL_PARSER"
echo "   GPU Memory: ${VLLM_GPU_MEMORY_UTIL} (~45GB)"
echo "   Log: $LOG_FILE"
echo ""
echo "â³ Loading model (this takes 3-5 minutes)..."
echo "   You can monitor progress with: tail -f $LOG_FILE"
echo ""

# Start vLLM server
./start-vllm-server.sh 2>&1 | tee "$LOG_FILE"
