#!/bin/bash
# Qwen2.5-Coder-1.5B Fast Completion Model - vLLM Server Launcher
# Port: 8001 | Model: ~1.5B parameters | VRAM: ~3GB | Context: 32K tokens
# Optimized for: Fast completions, autocomplete, quick code generation

set -e

echo "âš¡ Starting Qwen Fast Completion Model (1.5B)..."
echo "==============================================="

# Configuration
# HuggingFace Token (for gated models)
# Set via: export HF_TOKEN="your_token_here" or run scripts/setup-hf-token.sh
export HF_TOKEN="${HF_TOKEN:-}"
export VLLM_MODEL="Qwen/Qwen2.5-Coder-1.5B-Instruct"
export VLLM_SERVED_MODEL_NAME="qwen-autocomplete"
export VLLM_PORT=8001
export VLLM_HOST="0.0.0.0"
export VLLM_MAX_MODEL_LEN=32768  # 32K tokens
export VLLM_GPU_MEMORY_UTIL=0.3  # Use 30% of remaining VRAM (~3GB)
export VLLM_TOOL_PARSER="qwen"
# API Key will be generated by start-vllm-server.sh if not set
export VLLM_API_KEY="${VLLM_API_KEY:-}"

# Cache configuration
export HF_HOME="/workspace/hf-cache"
export HF_HUB_CACHE="/workspace/hf-cache"

# Logging
LOG_DIR="/workspace/logs"
mkdir -p "$LOG_DIR"
LOG_FILE="$LOG_DIR/autocomplete.log"

echo "ðŸ“‹ Configuration:"
echo "   Model: $VLLM_MODEL"
echo "   Served as: $VLLM_SERVED_MODEL_NAME"
echo "   Port: $VLLM_PORT"
echo "   Context Length: $VLLM_MAX_MODEL_LEN tokens"
echo "   Tool Parser: $VLLM_TOOL_PARSER"
echo "   GPU Memory: ${VLLM_GPU_MEMORY_UTIL} (~3GB)"
echo "   Log: $LOG_FILE"
echo ""
echo "â³ Loading model (this takes 1-2 minutes)..."
echo "   You can monitor progress with: tail -f $LOG_FILE"
echo ""

# Start vLLM server
./start-vllm-server.sh 2>&1 | tee "$LOG_FILE"
