# LLM Hosting on RunPod# Project Summary - GPT-OSS-20B Hosting



Self-hosted LLM inference server running on RunPod GPU instances with vLLM. Optimized for use with Continue.dev, GitHub Copilot, and other OpenAI-compatible clients.**Last Updated:** January 20, 2025 at 4:30 PM EST



## ğŸš€ Quick Start## Current Status: âœ… DEPLOYED on RunPod!



**Read [START-HERE.md](START-HERE.md) for complete setup instructions.****Pod ID:** `v5brcrgoclcp1p` | **GPU:** RTX 6000 Ada (48 GB) | **Cost:** $0.78/hr



### Current SetupğŸš€ **Active Deployment:** RunPod RTX 6000 Ada is now running and ready for vLLM deployment.

- **Platform**: RunPod (RTX 6000 Ada, 48GB VRAM)

- **Server**: vLLM v0.12.0 with OpenAI-compatible API**Quick Access:**

- **Primary Model**: GPT-OSS-20B (128K context, MXFP4 quantization)- ğŸ“Š **Jupyter Lab**: [Open Notebook](https://v5brcrgoclcp1p-8888.proxy.runpod.net/?token=42z5ic3ocbugvss4iuqr)

- **Client**: Continue.dev in VS Code- ğŸ’» **Web Terminal**: [Open Terminal](https://v5brcrgoclcp1p-19123.proxy.runpod.net/b6472jb6g7sa0bdoxdie9l6jrcen530q/)

- ğŸ“– **Full Details**: See `RUNPOD-DEPLOYED.md`

### Start a Model

## ğŸ“ Project Files

```bash

# From your local machine, start on RunPod:### ğŸ¯ Active Deployment

cd models/1. **`RUNPOD-DEPLOYED.md`** ğŸš€ ACTIVE

./gptoss.sh      # GPT-OSS-20B (OpenAI OSS model)   - **Current pod details and access URLs**

./qwen.sh        # Qwen 3 Coder 30B (best for coding)   - Complete vLLM deployment instructions

./gemma3.sh      # Gemma 3 27B (Google's model)   - Performance specs and cost tracking

./kimi-k2.sh     # Kimi K2 / DeepSeek-V3   - Troubleshooting guide

```   - **START HERE for next steps!**



All models use the generic `scripts/start-vllm-server.sh` launcher with optimized settings.### Main Documentation

2. **`GPT-OSS-20B_DigitalOcean_0.76hr.md`**

## ğŸ“ Repository Structure   - Original setup guide for DigitalOcean

   - âš ï¸ Updated with availability warning

```   - Still valid once GPUs are back in stock

llm-hosting/

â”œâ”€â”€ README.md                 # This file3. **`Alternative-GPU-Providers.md`**

â”œâ”€â”€ START-HERE.md             # Complete setup guide   - Complete guide to alternative cloud providers

â”œâ”€â”€ models/                   # Model launcher scripts   - RunPod, Vast.ai, Paperspace, Lambda Labs

â”‚   â”œâ”€â”€ gptoss.sh            # GPT-OSS-20B   - Detailed pricing comparison and setup guides

â”‚   â”œâ”€â”€ qwen.sh              # Qwen 3 Coder 30B

â”‚   â”œâ”€â”€ qwen3.sh             # Alternative Qwen launcher4. **`RUNPOD-DEPLOYMENT-GUIDE.md`**

â”‚   â”œâ”€â”€ gemma3.sh            # Gemma 3 27B   - Step-by-step RunPod deployment walkthrough

â”‚   â””â”€â”€ kimi-k2.sh           # Kimi K2 (DeepSeek-V3)   - Account setup and configuration

â”œâ”€â”€ scripts/                  # Utility scripts   - General RunPod reference guide

â”‚   â”œâ”€â”€ start-vllm-server.sh # Generic vLLM launcher

â”‚   â”œâ”€â”€ stop-server.sh       # Stop running server### Monitoring Tools

â”‚   â”œâ”€â”€ check-server.sh      # Health check5. **`check-do-gpu-availability.sh`** (Executable)

â”‚   â”œâ”€â”€ monitor-runpod.sh    # Monitor GPU usage   - Automated GPU availability checker

â”‚   â””â”€â”€ connect-runpod.sh    # SSH to RunPod   - Monitors DigitalOcean for RTX 4000 ADA restocking

â”œâ”€â”€ docs/                     # Documentation   - Sends macOS notifications when available

â”‚   â”œâ”€â”€ setup/               # Setup guides   - Can run continuously or as cron job

â”‚   â”œâ”€â”€ troubleshooting/     # Common issues & fixes

â”‚   â””â”€â”€ reference/           # API docs, file guide6. **`API-TOKEN-SETUP.md`**

â””â”€â”€ archive/                  # Old/obsolete files   - Instructions for setting up DigitalOcean API access

    â”œâ”€â”€ scripts/             # Archived test scripts   - How to use the monitoring script

    â””â”€â”€ docs/                # Outdated status docs   - Troubleshooting guide

```

## ğŸ¯ Next Steps: Deploy GPT-OSS-20B

## ğŸ”§ Key Features

### âœ… Pod is Ready - Deploy vLLM Now!

### Models

- **GPT-OSS-20B**: OpenAI's open-source model with native tool calling1. **Open Web Terminal**:

- **Qwen 3 Coder 30B**: Specialized for code generation   - Click: https://v5brcrgoclcp1p-19123.proxy.runpod.net/b6472jb6g7sa0bdoxdie9l6jrcen530q/

- **Gemma 3 27B**: Google's efficient instruction model

- All models support 128K context with FP8/MXFP4 quantization2. **Follow the guide in `RUNPOD-DEPLOYED.md`**:

   - Verify GPU with `nvidia-smi`

### Performance Optimizations   - Install vLLM: `pip install vllm`

- âœ… Prefix caching enabled (reuse context across requests)   - Deploy GPT-OSS-20B with 4-bit quantization

- âœ… Auto tokenizer mode (proper chat template detection)   - Test inference API

- âœ… CUDA graph optimization

- âœ… 95% GPU memory utilization3. **Estimated Time**: 15-20 minutes

- âœ… Chunked prefill for long contexts   - vLLM install: ~2 minutes

   - Model download: ~10-15 minutes (16 GB quantized)

### Continue.dev Integration   - First inference test: ~1 minute

- 13 stop sequences to prevent token leakage

- Temperature 0.1 for deterministic behavior### Alternative: Use Jupyter Lab

- 96K output tokens (131K total context)- URL: https://v5brcrgoclcp1p-8888.proxy.runpod.net/?token=42z5ic3ocbugvss4iuqr

- Tool use enabled with MCP servers- Open "New" â†’ "Terminal" for shell access

- Same deployment steps as web terminal

## ğŸ“– Documentation

## ğŸ“Š Deployment Comparison

### Setup Guides

- [Continue.dev Setup](docs/setup/CONTINUE-SETUP.md) - VS Code extension configuration### âœ… DEPLOYED: RunPod RTX 6000 Ada - $0.78/hr

- [RunPod Deployment](docs/setup/RUNPOD-DEPLOYMENT-GUIDE.md) - Cloud GPU setup- **Status:** Active and running now

- [API Configuration](docs/reference/API-KEY-GUIDE.md) - Authentication setup- **VRAM:** 48 GB (3x more headroom!)

- **Availability:** Immediate

### Troubleshooting- **Spot Option:** $0.39/hr (50% cheaper)

- [Token Leakage Fix](docs/troubleshooting/GPT-OSS-TOKEN-LEAKAGE-FIX.md) - **Recent fix!**

- [Common Issues](docs/troubleshooting/GPT-OSS-TROUBLESHOOTING.md)### â¸ï¸ Alternative: DigitalOcean RTX 4000 ADA - $0.76/hr

- [Copilot Integration](docs/troubleshooting/COPILOT-MODEL-ERROR-FIX.md)- **Status:** Limited availability (TOR1 only)

- May require waitlist/approval

### Reference- 20 GB VRAM

- [File Guide](docs/reference/FILE-GUIDE.md) - What each file does- Follow setup in `GPT-OSS-20B_DigitalOcean_0.76hr.md`

- [Server Management](docs/reference/SERVER-MANAGEMENT.md) - Start, stop, monitor

- [Tool Parser Research](docs/reference/TOOL-PARSER-RESEARCH.md) - Model capabilities### Option C: Ultra Budget Option

âœ… **Use RunPod RTX A6000 - $0.33/hr**

## ğŸ› ï¸ Common Tasks- Less than HALF the price!

- Still has 48 GB VRAM

### Check Server Status- Perfect for testing or budget projects

```bash

scripts/check-server.sh## ğŸ’° Cost Comparison

```

| Provider | GPU | Price/hr | Monthly (160h) | Status |

### Monitor GPU Usage|----------|-----|----------|----------------|--------|

```bash| **RunPod** | RTX 6000 Ada | $0.74 | $118 | âœ… **AVAILABLE NOW** |

scripts/monitor-runpod.sh| **RunPod** | RTX A6000 | $0.33 | $53 | âœ… **BEST VALUE** |

```| **RunPod** | L40 | $0.69 | $110 | âœ… Available |

| DigitalOcean | RTX 4000 Ada | $0.76 | $122 | âŒ Unavailable |

### View Logs

```bash## ğŸš€ Quick Start Commands

scripts/fetch-logs.sh

# or on RunPod:### Check DigitalOcean Availability

tail -f /workspace/logs/vllm-server.log```bash

```cd /Users/jsirish/AI/llm-hosting

./check-do-gpu-availability.sh check

### Stop Server```

```bash

scripts/stop-server.sh### Monitor Continuously (Get Notified)

# or on RunPod:```bash

kill $(cat /workspace/logs/vllm-server.pid)./check-do-gpu-availability.sh monitor

```# Checks hourly, notifies when available

```

## ğŸ” Security

### Start with RunPod Instead

- API keys are randomly generated per server start1. Go to [runpod.io](https://www.runpod.io)

- Keys saved to `/workspace/logs/api-key.txt` on RunPod2. Sign up and add $10 credit

- Update your Continue.dev config with new key after restart3. Deploy GPU Pod â†’ Choose RTX 6000 Ada or RTX A6000

- Config location: `~/.continue/config.yaml`4. Follow vLLM setup from main guide



## ğŸ’¡ Tips## ğŸ“Š What Can Run GPT-OSS-20B?



1. **First Time Setup**: Read START-HERE.md completelyMinimum requirements for 4-bit quantization:

2. **Model Choice**: Use Qwen for coding, GPT-OSS for general + tools- âœ… 20+ GB VRAM (model fits in ~16 GB)

3. **Context Length**: All models support 128K tokens (huge!)- âœ… CUDA-capable GPU

4. **Monitoring**: Check GPU memory with `monitor-runpod.sh`- âœ… vLLM support

5. **Logs**: Always check logs if something isn't working

Tested and working:

## ğŸ“Š Current Status- RTX 4000 Ada (20 GB) - Target but unavailable

- RTX 6000 Ada (48 GB) - **RECOMMENDED** âœ…

âœ… **Working**:- L40/L40S (48 GB) - Excellent âœ…

- GPT-OSS-20B running on RunPod- RTX A6000 (48 GB) - Budget option âœ…

- Continue.dev integration- RTX 3090 (24 GB) - Works, tight on VRAM âœ…

- Token leakage fixed (Dec 12, 2025)- RTX 4090 (24 GB) - Works, tight on VRAM âœ…

- All model launchers updated with performance optimizations

## ğŸ“ Software Stack (Same for All Providers)

â³ **In Progress**:

- Documentation consolidationRegardless of provider, the setup is identical:

- Testing other models (Gemma, Kimi)

```bash

## ğŸ¤ Contributing# 1. Install vLLM

pip install vllm

This is a personal project for self-hosted LLM inference. Feel free to use as reference or fork for your own setup.

# 2. Run GPT-OSS-20B

## ğŸ“ Notesvllm serve openai/gpt-oss-20b \

  --quantization mxfp4 \

- **Cost**: ~$0.76-0.79/hr on RunPod (RTX 6000 Ada)  --tensor-parallel-size 1 \

- **Storage**: 304TB workspace (plenty for model caching)  --max-model-len 8192 \

- **VRAM**: 48GB (can run 30B+ models with quantization)  --gpu-memory-utilization 0.90

- **Network**: Models cached, no re-download needed

# 3. Access API at http://localhost:8000/v1

---# Works with LibreChat, Open WebUI, etc.

```

**Last Updated**: December 12, 2025  

**Current Pod**: petite_coffee_koi-migration (3clxt008hl0a3a)  ## ğŸ”— Useful Links

**Status**: âœ… Operational

### Cloud Providers
- [RunPod](https://www.runpod.io) - **Recommended**
- [Vast.ai](https://vast.ai) - Cheapest
- [Paperspace](https://www.paperspace.com)
- [Lambda Labs](https://lambdalabs.com)
- [DigitalOcean](https://cloud.digitalocean.com/gpus/new) - When available

### Documentation
- [vLLM Documentation](https://docs.vllm.ai/)
- [GPT-OSS Model Card](https://huggingface.co/openai/gpt-oss-20b)
- [RunPod Documentation](https://docs.runpod.io/)

### Login Credentials
- DigitalOcean: Stored in 1Password (dev@dynamicagency.com)
- Access via: `op item get wnwfdxjyqxmp46xvds5qbsxwd4`

## â­ï¸ Next Actions

1. **Decide on provider:**
   - âœ… RunPod (available now, same price)
   - â³ Wait for DigitalOcean (unknown timeline)

2. **If using RunPod:**
   - Open `Alternative-GPU-Providers.md`
   - Follow "Quick Start: RunPod Setup" section
   - Choose RTX 6000 Ada ($0.74/hr) or RTX A6000 ($0.33/hr)

3. **If waiting for DigitalOcean:**
   - Set up API token (see `API-TOKEN-SETUP.md`)
   - Run: `./check-do-gpu-availability.sh monitor`
   - Get notified when available

4. **For testing/development:**
   - Use RunPod RTX A6000 at $0.33/hr
   - Test your workflow before committing to higher costs

## ğŸ“ Questions?

All documentation is in this folder. Key files:
- Main setup: `GPT-OSS-20B_DigitalOcean_0.76hr.md`
- Alternatives: `Alternative-GPU-Providers.md`
- Monitoring: `check-do-gpu-availability.sh`
- API setup: `API-TOKEN-SETUP.md`
